We thank the anonymous reviewers for their careful reading of our manuscript and their many insightful comments and suggestions.
We first present the overall response to the generla issues that these three reviewers have.


Overal response:

1) Contribution claims.  
We have largely modified the contribution claims of this manuscript. As the reviewers pointed out, we made inappropriate claims of our contributions. 
In this revision, we are careful to check and present each contribution claim in our paper. In specific, we reduced most of the contents related to the original framework.
We cited the references where an existing idea is used in this framework. We replace "framework" with "approach" in the entire manuscript. 

2) Presentation and writing style
We have largely improved the presentation quality and writing style. We add more related studies as references as the reviewers suggested. We reduced most of the contents in the framework description in order to achieve clarity and soundness. Since we have too many methods, we removed VSEncoding, Group-VSEncoding and SIMD-Group-VSEncoding. We revised the terminologies used throughout the paper.  

3) Evaluation on query processing performance.
Previously, the query processing time includes the loading time of posting lists from disks to main memory, which made the performance difference small and instable.
In this revision, we have entirely re-done the experiment on query processing evaluation. To reduce the disk IO disturbance, we first load all posting lists into the 
main memory. The time statistics were fully updated. 


REVIEWER #1:

1)  There are many, many schemes benchmarked in this paper, but there is no consistency and you are simply not sufficiently careful. 
We have Group-Simple benchmarks, but no Simple-8b benchmark even though Simple-8b is mentioned 3 times. It bothers me that the presentation is not consistent throughout. Sometimes you write Simple9, sometimes you write Simple-9. And so on.

Response: We have unified the algorithm names throughout the paper. And we a table to correspond abbreviations to the algorithm full names. We use Simple-9 because we found it performed similar to (even a bit better than) Simple-8b.

2) About the evaluation on the query processing performance.

Response:  The original time statics were made by including the loading time of posting lists from disks to main memory. Thus, the performance was not stable and inconsistent with previous evaluation experiments. Currently, to reduce the disk disturbance, we have loaded all the posting lists into the main memory. We following the suggestion that “I would either remove it or redo it entirely.”. We have re-run the experiments and organised the results by query rate. 

3) The applicability of our approach. 

Response: SIMD-FastPROR, PackedBinary and SIMD-BP128 can be implemented in this approach. Our approach is applicable to algorithms in which a control pattern encodes a sequence of integers.
[Add words on k-Gamma]


4) Your compression patterns are stored separately from the data area, but that is not really novel and you should put this trick into a greater context (e.g., that is how VSEncoding or FastPFOR work
if I understand correctly).

Response: We have modified the novelty claim. See the overall point.


	5) Optimisation tricks. 

Response: We have added the references where we have used an existing technique.  We still keep the optimisation details in order to make the reader easy to follow the key points in our approach.


	6) Please spell check (comparsion).

Response: We have largely improved the writing.



REVIEWER #2:



	Page 6, line 37: While it is not easy to vectorize traditional algorithm, it might be worth noting that it is possible, but results in more complex and slower alorithms. For example, see
	Thomas Willhalm, Nicolae Popovici, Yazan Boshmaf, Hasso Plattner, Alexander Zeier, and Jan Schaffner. 2009. SIMD-scan: ultra fast in-memory table scan using on-chip vector processing units. Proc. VLDB Endow. 2, 1 (August 2009), 385-394

Response: We remove this claim and cite the recommended paper.

	Page 6, line 39. It is a bold claim that others did not identify the fundamental problem in the parallelization of compression algorithms. In particular, since others like Schlegel or Lemire addressed it in their publications, and laid the foundation for your work. 

Response: We remove this claim.

	Page 7, Preliminaries: Apart from inverted index, similar techniques are used for used in data base compression, e.g.
	Vijayshankar Raman, Gopi Attaluri, Ronald Barber, Naresh Chainani, David Kalmuk, Vincent KulandaiSamy, Jens Leenstra, Sam Lightstone, Shaorong Liu, Guy M. Lohman, Tim Malkemus, Rene Mueller, Ippokratis Pandis, Berni Schiefer, David Sharpe, Richard Sidle, Adam Storm, and Liping Zhang. 2013. DB2 with BLU acceleration: so much more than just a column store. Proc. VLDB Endow. 6, 11 (August 2013), 1080-1091
	Christian Lemke, Kai-Uwe Sattler, Franz Faerber, and Alexander Zeier. 2010. Speeding up queries in column stores: a case for compression. In Proceedings of the 12th international conference on Data warehousing and knowledge discovery (DaWaK'10)
	Doug Inkster, Marcin Zukowski, and Peter Boncz. 2011. Integration of VectorWise with Ingres. SIGMOD Rec. 40, 3 (November 2011), 45-53


Response: We have cited these papers.


	Page 8, line 47: You state that your main idea is the separation of control from data sections. While this helps to simplify the application of SIMD, (1) this idea is not new and (2) it is not required. 

Response: We remove this claim.

	Page 9, Encoding Procedure. It is not clear to me how “S” is defined and what it does.

Response: Previously, the description was too general, and we largely reduced the contents here. 


	Page 9, line 55: “This step can be potentially parallelized by SIMD instructions.” This statement might be misinterpreted in that you see the potential for SIMD. My understanding is that you are doing this rearrangement of data for the sole purpose to allow a simple vectorization.

Response: We removed this claim. 

	Page 10, section 4.6: Your discussion focusses on the number of memory operations. This seems to be too narrow. Current processors can perform 2 loads and 1 store per cycle, so memory operations might not be the bottleneck and the reduction of other instructions might be of similar importance.

Response: We remove this claim. Currently, we only measure by the number of instructions, which is more clear.

	Page 10, line 52: It could be worth noting that your framework can easily be extended to 256 or 512 bit registers by using 8 or 16 data elements respectively. Please note that your processor also supports AVX with 256 bit (but lacks the vector shift instructions which requires support of AVX2.) Processors with AVX2 have been released a year ago and AVX-512 has been announced a year ago.

Response: Thanks for pointing this out.

	Page 11, line 11: You mention that step 1 cannot explicitly parallelized. Presenting this as “flexibility” is a strange way to position this short-coming.

Response: We remove this claim.

	Page 11, line 20: You state other architectures like ARM or PowerPC might use different vector size. It would have been better to verify this claim as both, Neon as well as AltiVec use 128 bit registers. On the other hand, the Intel instruction is begin extended to 256 and 512 bit registers.

Response:  Our experiments are limited by the available equipments of our Lab. Currently, our lab do not have such equipments to do the experiments 

	Page 12, line 33: You explain that you use a control byte, whereas in the details description on page 13, line 6, this is not mentioned anymore.

Response: We remove this claim.

	Page 12, line 39: The separation of pattern and data is not required. One could think of other schemes where the pattern selector with the data. Admittedly, it simplifies and thereby speeds up the processing.

Response: We remove this claim.

	Page 13, section 5.2.2: It is unnecessary to present the overview /after/ giving a precise description of the algorithm. I suggest to move this section up to the beginning of section 5.2.

Response:  We have made the revision as the suggestion.


	Page 15, line 39: The reduction of branches is a general optimization technique and not related to vectorization. It can also be applied to sequential code. In particular:
	(1)	Branches to compute the maximum can be avoided in sequential code by using conditional moves. 
	(2)	On assembly level, there is no such thing as a switch statement. To you mean to replace branches by a jump-table?
	The question remains, if you have applied these optimizations to the sequential code as well.

Response:  We have applied the similar similar techniques to both the scalar  and vectored algorithms. 
	[Add more explanations]


	Page 16, line 19. You define “Compression granularity” but then immediately use “Compression units” in line 25. Please align the two definitions.

Response: We remove “compression units”.


	Page 17, lines 17: The expression “can be stored across bytes” is not defined and leaves room for (mis-)interpretation. Do you mean that the compression scheme allows the descriptor to cross byte boundaries?

Response:  Yes, we have made it clear in the paper.

	Page 18, line 28: You seem not to be aware that the instructions “bit-scan reverse” and “bit-scan forward” have been part of the instruction set for a long time, and were later enhanced by leading-zero count. Ignoring these instructions in unary decoding leaves significant amount of performance on the table.

Response: [To be added]

	Page 18, Binary LD: Can you quantify the performance advantage of a table lookup in this case? Extracting a fixed number of bits can be implemented with 2 instructions (shift+and) and is therefore fairly efficient. Please note, that this can be “vectorized” with the BMI instruction “pdep”

Response: [To be added]


	Page 19, line 14: Why is this more cache efficient? The DCU and IP prefetchers should bring the data in L2 if not L1 cache.

Response: We removed this claim.

	Page 19, line 25/26. This is quite confusing. You first state that you store the bit offsets in memory, then you state that you keep them in registers.

Response: We have made the revision “they are kept in registers always”.

	Page 19, line 50: By “original implementation” do you mean “straight forward implementation”? Or are you referring to some specific implementation?

Response: We mean the SIMD-based implementation without optimisation techniques.

	Page 20, Section 7.1: The choice of names for M1 and M2 seems unmotivated. How about “w” for “widths” and “n” for “number”?

Response: We do this as suggested.



	Page 20, line 23: You strongly emphasize the speed-up of the encoding. It would be fair to point out that it comes at the cost of compression ratio (as measured in your analysis).

Response: Do it as suggested.

	Page 29, Section 8.6: You first elaborate that it is important to optimize for I/O, but then warm up the CPU caches in your measurements. This seems to be a contradiction as it rules out any I/O from the measurement. 
	Page 29, line 40: You conjecture that the performance hit is due to cache misses. It would be good to add data that supports this claim, e.g. by measuring the cache misses with tools like Linux perf, Intel PCM, or Likwid.

Response: All the experiments were entirely re-done. We carefully revised the contents correspondingly.


REVIEWER #3:

	1) The motivation of the paper could be improved. The paper argues that inverted indices are very large so that they have to be stored on disks. If the indices are stored on disks, however, then the disk’s bandwidth forms typically the main bottleneck (even for sequential access). Hence, the algorithms should not be CPU-bound, which makes the proposed optimizations useless. The main goal would then be to get a high compression ratio to reduce the bandwidth bottleneck.
	The paper should therefore argue that more and more data fits in main memory due to the availability of cheap main memory. Hence, the optimizations proposed in this paper are indeed very useful.

RESPONSE:  We have revised the introduction part as suggested.

	2) The paper has several grammatical errors and typos.
RESPONSE:  We have largely improved the paper.

	3) The paper exaggerates the novelty of the storage layout and encoding format. The vertical storage layout in combination with the separation of control and data codewords was already used in other papers. In my opinion, the paper’s main contributions are more the new vectorized algorithms that were implemented for the framework.

RESPONSE:  We have modified the claim and followed the suggestion.


	4) The paper moves the discussion of AVX2 and AVX-512 to future work, which I would love to have seen in this paper.

RESPONSE:  We are sorry that our lab currently does not support such processors.


	Detailed comments:
	-I would prefer the use of the serial (or oxford) comma. For example, instead of “how to store, index and search data” write “how to store, index, and search data”.

	-“i.e.” and “e.g.” within a sentence should be enclosed by two commas, like done in line 44 on page 3. If the authors do not want to use is in this way, they should at least make it consistent. 

	-Please used the center dot as multiplication sign instead of the * or the cross product sign. At least, one sign should be consistently used. For example, the * is used in line 46 on page 4 (and several other times) while the cross product sign is used in formula 2 in line 6 on page 18.

RESPONSE:  We have revised these parts.


	-Missing related work: Thomas Willhalm et al. “SIMD-Scan: Ultra Fast in-Memory Table Scan using on-chip vector processing units.” VLDB09 – The paper discusses a vectorized bit-packing.

RESPONSE:  We have cited this paper.


	-“Compression ratio” is used but newer introduced.

RESPONSE:  We added the explanation in the related work part.

	-“Time efficiency” sounds weird.

RESPONSE:  We have removed this.


	-Figure 2 on page 9 is unclear to me. What is the mapping between c) and d). It looks like that c) is in the horizontal data layout and d) is in the vertical data layout.

RESPONSE:  We have removed this.

	-What means “src and dst cannot be memory address at the same time” at the explanation of the MOVDQA instruction on page 10?

	RESPONSE:  We referred to assembly instructions like “CMD src dst”. Here we mean that at least one of “src” and “dst” is a register.

	-What is meant with “hard” modern processors on line 51 on page 10?

RESPONSE:  We have removed this.


	-The presentation of the pattern selection algorithm on page 13 is poor. There are several C/C++ specific notations (e.g., left-shift, pointer increment, de-reference operator in line 55) so that I would recommend using either (1) directly C as language or (2) a textual algorithm description. The current presentation does not support an easy understanding of the algorithm.

RESPONSE:  We have revised the part as suggested.

	-I would prefer to right-align the values in all tables of the experimental section, i.e., using the alignment as used in Table IV. This eases the comparison of the values.

RESPONSE:  We have revised the part as suggested.


	-On page 26 is mentioned that the shared pattern will improve the hit rate of the CPU cache. Why does the hit rate even matter when the algorithm has a sequential access pattern? The prefetcher of the CPU will take care that always all required cache lines are in the cache when the data is sequentially accessed from main memory. Or is the access pattern unpredictable for the prefetcher? Some more details, i.e., statistics from a profiler like VTune, would help.

RESPONSE:  We have removed this claim.


